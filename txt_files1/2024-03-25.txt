CS 214 / Multithreading
=======================

Threads
-------

"task" - instance of a fetch-execute cycle
    - has an instruction pointer, usually a stack

"process" - usually has one task
    - private virtual memory

a multitasking system can run more than one task concurrently
    can switch between multiple tasks on a single processor
    can schedule tasks between multiple processors (multiprocessing)


processes with separate virtual memories are protected from each other
but cannot easily communicate

"threads" - tasks within a single process
    a multithreaded process has multiple tasks sharing the same
        virtual memory

multithreading greatly simplifies communication between tasks
multithreading allows for miscommunication/interference between tasks


two major kinds of multithreading

"OS threads"/"kernel threads"
    - threads visible to OS
    - scheduling handled by OS
    - OS can schedule threads on separate processors


"green threads"/"library threads"/"language threads"
    - threading implemented within programming language 
    - process handles scheduling of its own threads
    - cannot take advantage of multiple processors
    - typically lower overhead than OS threads


Need for synchronization
------------------------

race condition
    - outcome of program depends on which thread finishes first
    - nondeterminism introduced by thread scheduling
    
        thread A        thread B
        --------        --------
        X = 1           X = 2
        
    program starts both threads, waits for both to finish, then
    prints X
        Does it print 1 or 2?
        Yes! Instead of a single outcome, we have a set of possible
        outcomes (nondeterminism)

data race
    - multiple threads making unsynchronized access to a piece of data,
        at least one of which writes to it
        
C programs containing data races are undefined!


    Thread A                                Thread B
    --------                                --------
    for (i = 0; i < 100; i++) {             X = 0
        X = 1;
        a[i] = X;
    }

101 possible outcomes, depending on when thread B runs  
    
    Thread A (optimized)
    --------------------
    X = 1;
    for (i = 0; i < 100; i++) {
        a[i] = X;
    }

101 possible outcomes, depending on when thread B runs
    -- with 2 exceptions, all of these are different from the
        optimized outcomes

we want to allow optimizations like these, so C declares data
    races to have undefined behavior

solution: don't have data races
    - forbid shared mutable data at language level (Rust)
    - synchronize access to shared data
        - enforce sequential access


for example, a memory fence is an instruction that pauses a thread
    until all pending writes have finished
    
    knowing when to use a memory fence can be difficult
    - using too many fences slows down our threads
    - not using the right fences can lead to data races
    
    compilers aren't (currently) smart enough to know where memory
        fences are needed
    typically, these are inserted in library code by experts


Tools for synchronization
-------------------------

mutual exclusion / "locks"
    idea: resources that at most one thread can access simultaneously


e.g., a shared queue
    only one thread can enqueue or dequeue at a time,
    but otherwise they can be scheduled freely


how can we make a lock?

int lock = 0;


example:
    while (lock == 1) { do nothing }
    lock = 1;
    // use shared resource
    lock = 0;

this doesn't work!
another thread could acquire the lock between exiting the loop and
    setting lock = 1
    
problem: the operation is not atomic
    other threads can be scheduled between the test and the set
    

one common solution: atomic hardware operations
    test-and-set
    compare-and-swap
    fetch-and-add

test-and-set is an atomic operation that checks the current value
of a variable and sets it

we can think of this as a function that sets a variable and returns
its old value

    while (test_and_set(&lock) == 1) { // do nothing }
    // do work
    lock = 0;
    


this sort of lock is called a "spinlock"
- horribly inefficient
- don't allow the scheduler to know when a thread is blocked by
    a lock
- requires a lot of bus traffic (needed to ensure the atomic nature
    of test-and-set)

instead of using spinlocks, we use higher-level system-provided
    mutex locks


The PThread (Posix Threads) library provides a mutex abstraction
that can be locked and unlocked

- only one thread can have a given lock at a time
- a thread attempting to lock a locked mutex will block until the
    mutex is unlocked


two operations: lock and unlock


    lock(&m);
    // do stuff    <-- critical section
    unlock(&m);

idea: enforce sequential access to shared data structures
only the thread holding the lock has access
    -> other threads must wait for that thread to finish before
        they can get access

C (Posix) gives us the tools to synchronize access to data
    structures, but it is up to us to use these tools correctly


nothing stops us from accessing a shared data structure without first
    getting exclusive access (except our own good sense)



next time:
    pthread_create
    pthread_join
    pthread_mutex_init
    pthread_mutex_lock
    pthread_mutex_unlock
